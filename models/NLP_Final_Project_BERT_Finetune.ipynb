{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFpKTD5Wnx1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcec292-8e57-417a-9366-c1c9373f6c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "# ================== IMPORT ==================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/NLP_Final_Project/train_clean.csv')\n",
        "test_df  = pd.read_csv('/content/drive/MyDrive/NLP_Final_Project/test_clean.csv')"
      ],
      "metadata": {
        "id": "LEQnaYr5bkWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_COL = \"text_clean\"\n",
        "LABEL_COL = \"label\""
      ],
      "metadata": {
        "id": "yI5nlSB6oAkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "ciNuTz7paXip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d59ba9c-dfbf-437a-a29f-93f8ffa1b253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= 1. Prepare tokenizer & dataset =========\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "MAX_LEN = 128"
      ],
      "metadata": {
        "id": "2-IIgVoUag2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== DATASET CLASS ==================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df, text_col, label_col, tokenizer, max_len=128):\n",
        "        self.texts = df[text_col].astype(str).tolist()\n",
        "        self.labels = df[label_col].astype(int).tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "ILz59rRnal3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loader(dataset, batch_size, shuffle=False):\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
      ],
      "metadata": {
        "id": "n_S42U1BgDXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_sub, val_sub = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=train_df[LABEL_COL],\n",
        ")"
      ],
      "metadata": {
        "id": "xfamdDVxgFAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classes = np.sort(train_df[LABEL_COL].unique())\n",
        "num_labels = len(classes)\n",
        "print(\"Classes:\", classes)\n",
        "print(\"Number of labels:\", num_labels)\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=classes,\n",
        "    y=train_sub[LABEL_COL].values,\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "print(\"Class weights:\", class_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1EFHpPNgHPB",
        "outputId": "2cd07d2f-c90e-4301-9147-700e5a2bb744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: [0 1 2]\n",
            "Number of labels: 3\n",
            "Class weights: tensor([0.9732, 0.8180, 1.3333])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets\n",
        "train_dataset = TextDataset(train_sub, TEXT_COL, LABEL_COL, tokenizer, MAX_LEN)\n",
        "val_dataset   = TextDataset(val_sub,   TEXT_COL, LABEL_COL, tokenizer, MAX_LEN)\n",
        "test_dataset  = TextDataset(test_df,   TEXT_COL, LABEL_COL, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "YCf1f2yNgI9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== TRAIN 1 MODEL ==================\n",
        "def train_one_model(\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_labels,\n",
        "    lr=2e-5,\n",
        "    epochs=3,\n",
        "    class_weights=None,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "):\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=num_labels,\n",
        "        hidden_dropout_prob=0.3,   # default = 0.1\n",
        "        attention_probs_dropout_prob=0.3\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    num_warmup_steps = int(warmup_ratio * total_steps)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    if class_weights is not None:\n",
        "        cw = class_weights.to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=cw)\n",
        "    else:\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "\n",
        "            loss = loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": total_loss / (pbar.n + 1)})\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f\"[Epoch {epoch+1}] Avg train loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # ----- EVAL -----\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                )\n",
        "                logits = outputs.logits\n",
        "                loss = loss_fn(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_macro_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
        "        print(\n",
        "            f\"[Epoch {epoch+1}] Val loss: {avg_val_loss:.4f} | \"\n",
        "            f\"Val Macro F1: {val_macro_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    final_macro_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
        "    return model, final_macro_f1"
      ],
      "metadata": {
        "id": "im0rTRrpamwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== GRID SEARCH ==================\n",
        "import itertools\n",
        "\n",
        "param_grid = {\n",
        "    \"lr\": [1.5e-5, 2e-5],\n",
        "    \"batch_size\": [16],\n",
        "    \"epochs\": [2, 3],\n",
        "    \"weight_decay\": [0.01, 0.02],\n",
        "    \"warmup_ratio\": [0.06],\n",
        "}"
      ],
      "metadata": {
        "id": "lrxh6fUYoBIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_f1 = -1.0\n",
        "best_params = None\n",
        "best_model = None"
      ],
      "metadata": {
        "id": "xMeqLVN3oZUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lr, batch_size, epochs, weight_decay, warmup_ratio in itertools.product(\n",
        "    param_grid[\"lr\"],\n",
        "    param_grid[\"batch_size\"],\n",
        "    param_grid[\"epochs\"],\n",
        "    param_grid[\"weight_decay\"],\n",
        "    param_grid[\"warmup_ratio\"],\n",
        "):\n",
        "    print(\"=\" * 60)\n",
        "    print(\n",
        "        f\"Trying params: lr={lr}, batch_size={batch_size}, \"\n",
        "        f\"epochs={epochs}, weight_decay={weight_decay}, \"\n",
        "        f\"warmup_ratio={warmup_ratio}\"\n",
        "    )\n",
        "\n",
        "    train_loader = make_loader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = make_loader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model, val_f1 = train_one_model(\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_labels=num_labels,\n",
        "        lr=lr,\n",
        "        epochs=epochs,\n",
        "        class_weights=class_weights,\n",
        "        weight_decay=weight_decay,\n",
        "        warmup_ratio=warmup_ratio,\n",
        "    )\n",
        "\n",
        "    print(f\"Params -> Val Macro F1 = {val_f1:.4f}\")\n",
        "\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        best_params = {\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"epochs\": epochs,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"warmup_ratio\": warmup_ratio,\n",
        "        }\n",
        "        best_model = model\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Best params:\", best_params)\n",
        "print(\"Best Val Macro F1:\", best_f1)"
      ],
      "metadata": {
        "id": "wRYfhhWFoq8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5abee30-8c0d-41dd-ce0f-05a2c6db88b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Trying params: lr=1.5e-05, batch_size=16, epochs=2, weight_decay=0.01, warmup_ratio=0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/2: 100%|██████████| 50/50 [00:18<00:00,  2.77it/s, loss=1.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg train loss: 1.0533\n",
            "[Epoch 1] Val loss: 0.9453 | Val Macro F1: 0.4589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 50/50 [00:17<00:00,  2.78it/s, loss=0.86]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg train loss: 0.8600\n",
            "[Epoch 2] Val loss: 0.8304 | Val Macro F1: 0.4983\n",
            "Params -> Val Macro F1 = 0.4983\n",
            "============================================================\n",
            "Trying params: lr=1.5e-05, batch_size=16, epochs=2, weight_decay=0.02, warmup_ratio=0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/2: 100%|██████████| 50/50 [00:18<00:00,  2.75it/s, loss=1.08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg train loss: 1.0814\n",
            "[Epoch 1] Val loss: 0.9708 | Val Macro F1: 0.7022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 50/50 [00:18<00:00,  2.68it/s, loss=0.903]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg train loss: 0.9026\n",
            "[Epoch 2] Val loss: 0.8518 | Val Macro F1: 0.7230\n",
            "Params -> Val Macro F1 = 0.7230\n",
            "============================================================\n",
            "Trying params: lr=1.5e-05, batch_size=16, epochs=3, weight_decay=0.01, warmup_ratio=0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/3: 100%|██████████| 50/50 [00:18<00:00,  2.72it/s, loss=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg train loss: 1.0046\n",
            "[Epoch 1] Val loss: 0.8470 | Val Macro F1: 0.5720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 50/50 [00:18<00:00,  2.71it/s, loss=0.777]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg train loss: 0.7770\n",
            "[Epoch 2] Val loss: 0.6985 | Val Macro F1: 0.6353\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 50/50 [00:18<00:00,  2.74it/s, loss=0.663]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Avg train loss: 0.6625\n",
            "[Epoch 3] Val loss: 0.6552 | Val Macro F1: 0.6781\n",
            "Params -> Val Macro F1 = 0.6781\n",
            "============================================================\n",
            "Trying params: lr=1.5e-05, batch_size=16, epochs=3, weight_decay=0.02, warmup_ratio=0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/3: 100%|██████████| 50/50 [00:18<00:00,  2.72it/s, loss=1.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg train loss: 1.0264\n",
            "[Epoch 1] Val loss: 0.9030 | Val Macro F1: 0.5618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 50/50 [00:18<00:00,  2.71it/s, loss=0.773]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg train loss: 0.7734\n",
            "[Epoch 2] Val loss: 0.7453 | Val Macro F1: 0.6844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 50/50 [00:18<00:00,  2.73it/s, loss=0.642]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Avg train loss: 0.6423\n",
            "[Epoch 3] Val loss: 0.6953 | Val Macro F1: 0.7239\n",
            "Params -> Val Macro F1 = 0.7239\n",
            "============================================================\n",
            "Trying params: lr=2e-05, batch_size=16, epochs=2, weight_decay=0.01, warmup_ratio=0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/2: 100%|██████████| 50/50 [00:18<00:00,  2.71it/s, loss=0.984]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg train loss: 0.9840\n",
            "[Epoch 1] Val loss: 0.8450 | Val Macro F1: 0.4635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 50/50 [00:18<00:00,  2.73it/s, loss=0.697]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg train loss: 0.6972\n",
            "[Epoch 2] Val loss: 0.6574 | Val Macro F1: 0.7170\n",
            "Params -> Val Macro F1 = 0.7170\n",
            "============================================================\n",
            "Trying params: lr=2e-05, batch_size=16, epochs=2, weight_decay=0.02, warmup_ratio=0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/2: 100%|██████████| 50/50 [00:18<00:00,  2.73it/s, loss=1.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg train loss: 1.0224\n",
            "[Epoch 1] Val loss: 0.8419 | Val Macro F1: 0.4878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 50/50 [00:18<00:00,  2.71it/s, loss=0.752]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg train loss: 0.7517\n",
            "[Epoch 2] Val loss: 0.7217 | Val Macro F1: 0.6501\n",
            "Params -> Val Macro F1 = 0.6501\n",
            "============================================================\n",
            "Trying params: lr=2e-05, batch_size=16, epochs=3, weight_decay=0.01, warmup_ratio=0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/3: 100%|██████████| 50/50 [00:18<00:00,  2.72it/s, loss=1.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg train loss: 1.0579\n",
            "[Epoch 1] Val loss: 0.8771 | Val Macro F1: 0.6998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 50/50 [00:18<00:00,  2.70it/s, loss=0.693]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg train loss: 0.6927\n",
            "[Epoch 2] Val loss: 0.5596 | Val Macro F1: 0.7700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 50/50 [00:18<00:00,  2.73it/s, loss=0.541]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Avg train loss: 0.5407\n",
            "[Epoch 3] Val loss: 0.5051 | Val Macro F1: 0.8077\n",
            "Params -> Val Macro F1 = 0.8077\n",
            "============================================================\n",
            "Trying params: lr=2e-05, batch_size=16, epochs=3, weight_decay=0.02, warmup_ratio=0.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1/3: 100%|██████████| 50/50 [00:18<00:00,  2.72it/s, loss=1.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Avg train loss: 1.0342\n",
            "[Epoch 1] Val loss: 0.8481 | Val Macro F1: 0.6943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 50/50 [00:18<00:00,  2.72it/s, loss=0.717]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Avg train loss: 0.7168\n",
            "[Epoch 2] Val loss: 0.6544 | Val Macro F1: 0.6624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 50/50 [00:18<00:00,  2.72it/s, loss=0.576]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Avg train loss: 0.5762\n",
            "[Epoch 3] Val loss: 0.6032 | Val Macro F1: 0.6756\n",
            "Params -> Val Macro F1 = 0.6756\n",
            "============================================================\n",
            "Best params: {'lr': 2e-05, 'batch_size': 16, 'epochs': 3, 'weight_decay': 0.01, 'warmup_ratio': 0.06}\n",
            "Best Val Macro F1: 0.8077079663339205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_loader(model, loader, name=\"\"):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            y = batch[\"labels\"].to(device)\n",
        "\n",
        "            logits = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            ).logits\n",
        "\n",
        "            p = torch.argmax(logits, dim=1)\n",
        "\n",
        "            preds.extend(p.cpu().numpy())\n",
        "            labels.extend(y.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n=== Performance on {name} ===\")\n",
        "    print(classification_report(labels, preds, digits=4))\n",
        "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    print(\"Macro F1:\", macro_f1)\n",
        "    return labels, preds, macro_f1"
      ],
      "metadata": {
        "id": "dY-Fjam1ou7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = best_params[\"batch_size\"]\n",
        "\n",
        "train_loader_dbg = make_loader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = make_loader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Train_sub\n",
        "train_labels, train_preds, train_f1 = evaluate_on_loader(\n",
        "    best_model, train_loader_dbg, name=\"TRAIN_SUB\"\n",
        ")\n",
        "\n",
        "# Test_set\n",
        "test_labels, test_preds, test_f1 = evaluate_on_loader(\n",
        "    best_model, test_loader, name=\"TEST_SET\"\n",
        ")\n",
        "\n",
        "print(\"\\nConfusion matrix (TEST_SET):\")\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HVFQRkCohfA",
        "outputId": "dac91896-f0c0-45fc-af34-ccb6af393376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Performance on TRAIN_SUB ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8626    0.8248    0.8433       274\n",
            "           1     0.7890    0.7914    0.7902       326\n",
            "           2     0.8199    0.8650    0.8418       200\n",
            "\n",
            "    accuracy                         0.8213       800\n",
            "   macro avg     0.8238    0.8271    0.8251       800\n",
            "weighted avg     0.8219    0.8213    0.8213       800\n",
            "\n",
            "Macro F1: 0.8251106038906343\n",
            "\n",
            "=== Performance on TEST_SET ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8286    0.8208    0.8246       106\n",
            "           1     0.7975    0.7730    0.7850       163\n",
            "           2     0.5965    0.6667    0.6296        51\n",
            "\n",
            "    accuracy                         0.7719       320\n",
            "   macro avg     0.7408    0.7535    0.7464       320\n",
            "weighted avg     0.7757    0.7719    0.7734       320\n",
            "\n",
            "Macro F1: 0.7464403027882085\n",
            "\n",
            "Confusion matrix (TEST_SET):\n",
            "[[ 87  15   4]\n",
            " [ 18 126  19]\n",
            " [  0  17  34]]\n"
          ]
        }
      ]
    }
  ]
}